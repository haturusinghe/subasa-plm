{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Subasa: For Testing Final Model",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "600",
                "--patience", "3",
                "--batch_size", "1",
                "--seed", "42",
                "--wandb_project", "subasa-xlmr-base-debug",
                "--finetuning_stage", "final",
                "--num_labels", "2",
                "--dataset", "sold",
                "--test", "True",
                "--model_path", "final_finetune/08012025-1353_LK_xlm-roberta-base_mrp_2e-05_16_600_seed42_ncls2_final/08012025-1353_LK_xlm-roberta-base_mrp_2e-05_16_600_seed42_ncls2_final.ckpt"
            ]
        },
        {
            "name": "Subasa: For Training MRP Model",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--intermediate", "mrp",
                "--val_int", "1000",
                "--patience", "3",
                "--mask_ratio", "0.5",
                "--n_tk_label", "2",
                "--epochs", "5",
                "--batch_size", "16",
                "--lr", "0.00002",
                "--seed", "42",
                "--wandb_project", "subasa-xlmr-base-debug",
                "--finetuning_stage", "pre",
                "--dataset", "sold",
                "--skip_empty_rat", "True",
                "--push_to_hub", "True",
                // "--check_errors", "False",
            ]
        },
        {
            "name": "Subasa: Train for OffensiveDetection (Final)",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "250",
                "--patience", "3",
                "--epochs", "5",
                "--batch_size", "16",
                "--lr", "0.00002",
                "--seed", "42",
                "--wandb_project", "subasa-xlmr-base-debug",
                "--finetuning_stage", "final",
                "--dataset", "sold",
                "--num_labels", "2",
                "--pre_finetuned_model", "pre_finetune/09012025-1343_LK_2e-05_16_250_seed42_xlm-roberta-base_mrp_pre/xlm-roberta-base_mrp_val_loss_0.115060_ep4_stp249_f1_0.625580.ckpt"
            ]
        },
        {
            "name": "Subasa: Testing Final Model After Finetuneing for OffensiveDetection (Final)",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "2500",
                "--patience", "3",
                "--epochs", "1",
                "--batch_size", "1",
                "--seed", "42",
                "--wandb_project", "subasa-xlmr-base-debug",
                "--finetuning_stage", "final",
                "--dataset", "sold",
                "--num_labels", "2",
                "--test", "True",
                "--test_model_path", "final_finetune/09012025-1650_LK_2e-05_16_250_seed42_final/xlm-roberta-base_final__val_loss_0.438462_ep4_stp249_f1_0.841653.ckpt"
            ]
        },

        {
            "name": "Subasa: Testing MRP Model after Pre-Finetuning Phase",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "2500",
                "--patience", "3",
                "--epochs", "1",
                "--batch_size", "1",
                "--seed", "42",
                "--wandb_project", "subasa-xlmr-base-debug",
                "--finetuning_stage", "pre",
                "--dataset", "sold",
                "--num_labels", "2",
                "--skip_empty_rat", "True",
                "--test", "True",
                "--intermediate", "mrp",
                "--test_model_path", "pre_finetune/09012025-1343_LK_2e-05_16_250_seed42_xlm-roberta-base_mrp_pre/xlm-roberta-base_mrp_val_loss_0.144085_ep4_stp0_f1_0.550156.ckpt"
            ]
        },

        {
            "name": "1.0 Mask Ratio (RP) Training",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "1000",
                "--patience", "3",
                "--epochs", "5",
                "--batch_size", "16",
                "--seed", "42",
                "--wandb_project", "subasa-xlmr-base-debug",
                "--finetuning_stage", "pre",
                "--dataset", "sold",
                "--n_tk_label", "2",
                "--skip_empty_rat", "True",
                "--intermediate", "rp",
                "--mask_ratio", "1.0",
                "--lr", "0.00002",
            ]
        },

        {
            "name": "MLM PRE- Training",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "1000",
                "--patience", "3",
                "--epochs", "5",
                "--batch_size", "2",
                "--seed", "42",
                "--wandb_project", "subasa-xlmr-base-session2",
                "--finetuning_stage", "pre",
                "--dataset", "sold",
                "--n_tk_label", "2",
                "--skip_empty_rat", "True",
                "--intermediate", "mlm",
                "--mask_ratio", "0.15",
                "--lr", "0.00002",
            ]
        },

        {
            "name": "Subasa: Testing MMLM",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "2500",
                "--patience", "3",
                "--epochs", "1",
                "--batch_size", "1",
                "--seed", "66",
                "--wandb_project", "ablataion-study-debug",
                "--finetuning_stage", "pre",
                "--dataset", "sold",
                "--n_tk_label", "2",
                "--skip_empty_rat", "True",
                "--intermediate", "mlm",
                "--test", "True",
                "--mask_ratio", "0.15",
                "--test_model_path", "xlm-roberta-base"
            ]
        },

        {
            "name": "Data Augment",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--skip", "True",
            ]
        },

        {
            "name": "XLMR Base Testing No Training",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--wandb_p", "sold_without_mrp_to_hs_v2",
                "--exp_save_name", "no_training_base",
                "--test_model_path", "FacebookAI/xlm-roberta-base",
                "--finetuning_stage", "final",
                "--dataset", "sold",
                "--seed", "42",
                "--mask_ratio", "0",
                "--test", "True",
                "--val_int", "10000",
                "--patience", "3",
                "--epochs", "1",
                "--batch_size", "1",
                "--lr", "0.00002",
                "--num_labels", "2",
                "--push_to_hub", "False",
                "--short_name", "True"
            ]
        }
        ,
        {
            "name": "XLMR Base Pre-Finetune Testing DEB",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--wandb_project", "xlmr-base-42-b16-e5-radam-s42-mrp-msk0.5",
                "--finetuning_stage", "pre",
                "--dataset", "sold",
                "--n_tk_label", "2",
                "--intermediate", "mrp",
                "--mask_ratio", "0.5",
                "--test", "True",
                "--test_model_path", "pre_finetune/mrp_0.5_seed42_b16_e5_radam_s42_msk0.5_2/ep5.ckpt",
                "--val_int", "2500",
                "--patience", "3",
                "--epochs", "1",
                "--batch_size", "1",
                "--seed", "42"
            ]
        },
        {
            "name": "AHHHHH",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args": [
                "--pretrained_model", "xlm-roberta-base",
                "--val_int", "10000",
                "--patience", "3",
                "--epochs", "5",
                "--batch_size", "16",
                "--lr", "0.00002",
                "--seed", "42",
                "--wandb_project", "debug-baseline-without-augmented-2-seed42",
                "--finetuning_stage", "final",
                "--dataset", "sold",
                "--num_labels", "2",
                "--mask_ratio", "0.5",
                "--pre_finetuned_model", "s-haturusinghe/mrp_0.5_seed42_ep5",
                "--exp_save_name", "final_w_mrp_0.5_seed42_ep3_ckpt",
                "--short_name", "True",
                "--push_to_hub", "True",
                "--intermediate", "mrp"
            ]
        }
        
        
    ]
}
